{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle5 as pickle\n",
    "import json\n",
    "import utils\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pyarrow as pa\n",
    "import networkx as nx\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "from sherlock import helpers\n",
    "from sherlock.deploy.model import SherlockModel\n",
    "from sherlock.functional import extract_features_to_csv\n",
    "from sherlock.features.paragraph_vectors import initialise_pretrained_model, initialise_nltk\n",
    "from sherlock.features.preprocessing import (\n",
    "    extract_features,\n",
    "    convert_string_lists_to_lists,\n",
    "    prepare_feature_extraction,\n",
    "    load_parquet_values,\n",
    ")\n",
    "from sherlock.features.word_embeddings import initialise_word_embeddings\n",
    "\n",
    "%load_ext autoreload"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialize Sherlock Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing feature extraction by downloading 4 files:\n",
      "        \n",
      " ../sherlock/features/glove.6B.50d.txt, \n",
      " ../sherlock/features/par_vec_trained_400.pkl.docvecs.vectors_docs.npy,\n",
      "        \n",
      " ../sherlock/features/par_vec_trained_400.pkl.trainables.syn1neg.npy, and \n",
      " ../sherlock/features/par_vec_trained_400.pkl.wv.vectors.npy.\n",
      "        \n",
      "All files for extracting word and paragraph embeddings are present.\n",
      "Initialising word embeddings\n",
      "Initialise Word Embeddings process took 0:00:12.503732 seconds.\n",
      "Initialise Doc2Vec Model, 400 dim, process took 0:00:47.488976 seconds. (filename = ../sherlock/features/par_vec_trained_400.pkl)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/aristotle/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/aristotle/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialised NLTK, process took 0:00:04.516164 seconds.\n"
     ]
    }
   ],
   "source": [
    "prepare_feature_extraction()\n",
    "initialise_word_embeddings()\n",
    "initialise_pretrained_model(400)\n",
    "initialise_nltk()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the pre-trained sherlock model with its default parameters \n",
    "model = SherlockModel();\n",
    "model.initialize_model_from_json(with_weights=True, model_id=\"sherlock\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run Sherlock on the Synthetic Benchmark (SB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting Features: 100%|██████████| 3/3 [00:00<00:00,  8.69it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exporting 1588 column features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  8%|▊         | 1/13 [00:00<00:04,  2.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['animal_name', 'animal_scientific_name', 'country'], dtype='object') ['species' 'species' 'country']\n",
      "Exporting 1588 column features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting Features: 100%|██████████| 2/2 [00:00<00:00,  7.98it/s]\n",
      "Extracting Features: 100%|██████████| 2/2 [00:00<00:00, 50.77it/s]\n",
      " 23%|██▎       | 3/13 [00:00<00:02,  3.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['animal_name', 'animal_scientific_name'], dtype='object') ['species' 'species']\n",
      "Exporting 1588 column features\n",
      "Index(['state', 'state_abbrev'], dtype='object') ['state' 'state']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exporting 1588 column features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting Features: 100%|██████████| 3/3 [00:00<00:00,  9.30it/s]\n",
      " 31%|███       | 4/13 [00:01<00:03,  2.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['company_name', 'full_name', 'country'], dtype='object') ['company' 'name' 'country']\n",
      "Exporting 1588 column features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting Features: 100%|██████████| 3/3 [00:00<00:00,  7.40it/s]\n",
      " 38%|███▊      | 5/13 [00:02<00:03,  2.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['plant_name', 'plant_scientific_name', 'plant_family'], dtype='object') ['name' 'name' 'family']\n",
      "Exporting 1588 column features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting Features: 100%|██████████| 2/2 [00:00<00:00,  7.47it/s]\n",
      " 46%|████▌     | 6/13 [00:02<00:02,  2.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['product_grocery', 'country'], dtype='object') ['description' 'country']\n",
      "Exporting 1588 column features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting Features: 100%|██████████| 4/4 [00:00<00:00, 13.76it/s]\n",
      " 54%|█████▍    | 7/13 [00:02<00:02,  2.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['car_make', 'car_model', 'car_model_year', 'country_code'], dtype='object') ['brand' 'name' 'year' 'country']\n",
      "Exporting 1588 column features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting Features: 100%|██████████| 5/5 [00:00<00:00, 17.89it/s]\n",
      " 62%|██████▏   | 8/13 [00:03<00:01,  2.64it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['first_name', 'last_name', 'ssn', 'gender', 'country'], dtype='object') ['name' 'name' 'address' 'sex' 'country']\n",
      "Exporting 1588 column features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting Features: 100%|██████████| 3/3 [00:00<00:00, 10.88it/s]\n",
      " 69%|██████▉   | 9/13 [00:03<00:01,  2.73it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['movie_title', 'movie_genre', 'country_code'], dtype='object') ['name' 'genre' 'country']\n",
      "Exporting 1588 column features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting Features: 100%|██████████| 4/4 [00:00<00:00, 18.06it/s]\n",
      "Extracting Features: 100%|██████████| 2/2 [00:00<00:00, 12.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['full_name', 'credit_card_type', 'credit_card', 'email_address'], dtype='object') ['name' 'class' 'address' 'address']\n",
      "Exporting 1588 column features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 85%|████████▍ | 11/13 [00:03<00:00,  3.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['city', 'country'], dtype='object') ['city' 'country']\n",
      "Exporting 1588 column features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting Features: 100%|██████████| 4/4 [00:00<00:00,  7.99it/s]\n",
      "Extracting Features: 100%|██████████| 2/2 [00:00<00:00, 14.89it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['plant_name', 'plant_scientific_name', 'plant_family', 'country'], dtype='object') ['name' 'name' 'family' 'country']\n",
      "Exporting 1588 column features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 13/13 [00:04<00:00,  2.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['country', 'country_code'], dtype='object') ['country' 'country']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "synthetic_benchmark_data_dir=\"data/synthetic_benchmark/\"\n",
    "graph = pickle.load(open('graph_representations/synthetic_benchmark/bipartite.graph', 'rb'))\n",
    "\n",
    "# Groundtruth Homographs\n",
    "gt = pd.read_pickle('ground_truth/synthetic_example_groundtruth_dict.pickle')\n",
    "gt_homographs = set([val for val in gt if gt[val]=='homograph'])\n",
    "\n",
    "# Find the semantic types for each column\n",
    "column_node_to_semantic_type_dict = utils.sherlock_helpers.get_column_node_to_semantic_type_dict(data_dir=synthetic_benchmark_data_dir, model=model)\n",
    "\n",
    "# Find the semantic types for each cell node\n",
    "cell_node_to_semantic_type_dict = utils.sherlock_helpers.get_cell_node_to_semantic_type(graph, column_node_to_semantic_type_dict)\n",
    "predicted_homographs = utils.sherlock_helpers.get_predicted_homographs(cell_node_to_semantic_type_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 1.0\n",
      "Recall: 0.6909090909090909\n",
      "F1-Score: 0.8172043010752689\n"
     ]
    }
   ],
   "source": [
    "precision = len(gt_homographs & predicted_homographs) / len(predicted_homographs)\n",
    "recall = len(gt_homographs & predicted_homographs) / len(gt_homographs)\n",
    "f1_score = (2* precision * recall) / (precision + recall)\n",
    "\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Recall:\", recall)\n",
    "print(\"F1-Score:\", f1_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GT homographs not predicted by Sherlock: {'Christophe', 'Nadine', 'Conroy', 'Elan', 'Heather', 'Reid', 'Garvey', 'Duff', 'Charity', 'Smitty', 'Leandra', 'Vinson', 'Jimmy', 'Crossfire', 'Berkeley', 'Costanza', 'Else'}\n"
     ]
    }
   ],
   "source": [
    "print(\"GT homographs not predicted by Sherlock:\", gt_homographs - predicted_homographs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run Sherlock on TUS Benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir=\"data/TUS/csvfiles/\"\n",
    "graph = pickle.load(open('graph_representations/TUS/bipartite.graph', 'rb'))\n",
    "\n",
    "# Groundtruth Homographs\n",
    "with open(\"ground_truth/groundtruth_TUS_short_format.pickle\", \"rb\") as fh:\n",
    "    gt = pickle.load(fh)\n",
    "gt_homographs = set([val for val in gt if gt[val]=='homograph'])\n",
    "\n",
    "column_node_to_semantic_type_dict = pd.read_pickle('output/TUS/column_node_to_semantic_type_dict.pickle')\n",
    "cell_node_to_semantic_type_dict = pd.read_pickle('output/TUS/cell_node_to_semantic_type_dict.pickle')\n",
    "predicted_homographs = pd.read_pickle('output/TUS/predicted_homographs.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.7449533939497913\n",
      "Recall: 0.5003648934127136\n",
      "F1-Score: 0.5986397683929966\n"
     ]
    }
   ],
   "source": [
    "precision = len(gt_homographs & predicted_homographs) / len(predicted_homographs)\n",
    "recall = len(gt_homographs & predicted_homographs) / len(gt_homographs)\n",
    "f1_score = (2* precision * recall) / (precision + recall)\n",
    "\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Recall:\", recall)\n",
    "print(\"F1-Score:\", f1_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26035\n",
      "17487\n"
     ]
    }
   ],
   "source": [
    "print(len(gt_homographs))\n",
    "print(len(predicted_homographs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'address', 'description', 'location'}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cell_node_to_semantic_type_dict['Music Faculty']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['CommonName_t_67c3f7ce5eab8804____c10_0____3.csv',\n",
       " 'Landmark_t_67c3f7ce5eab8804____c11_0____3.csv',\n",
       " 'Landmark_t_67c3f7ce5eab8804____c11_1____3.csv',\n",
       " 'CommonName_t_67c3f7ce5eab8804____c12_0____3.csv',\n",
       " 'Landmark_t_67c3f7ce5eab8804____c12_0____3.csv',\n",
       " 'CommonName_t_67c3f7ce5eab8804____c12_1____3.csv',\n",
       " 'Landmark_t_67c3f7ce5eab8804____c12_1____3.csv',\n",
       " 'CommonName_t_67c3f7ce5eab8804____c13_0____3.csv',\n",
       " 'Landmark_t_67c3f7ce5eab8804____c13_0____3.csv',\n",
       " 'CommonName_t_67c3f7ce5eab8804____c13_1____3.csv',\n",
       " 'Landmark_t_67c3f7ce5eab8804____c13_1____3.csv',\n",
       " 'CommonName_t_67c3f7ce5eab8804____c14_0____3.csv',\n",
       " 'CommonName_t_67c3f7ce5eab8804____c14_1____3.csv',\n",
       " 'Landmark_t_67c3f7ce5eab8804____c14_1____3.csv',\n",
       " 'CommonName_t_67c3f7ce5eab8804____c15_0____3.csv',\n",
       " 'Landmark_t_67c3f7ce5eab8804____c15_0____3.csv',\n",
       " 'CommonName_t_67c3f7ce5eab8804____c15_1____3.csv',\n",
       " 'Landmark_t_67c3f7ce5eab8804____c15_1____3.csv',\n",
       " 'CommonName_t_67c3f7ce5eab8804____c16_0____3.csv',\n",
       " 'Landmark_t_67c3f7ce5eab8804____c16_0____3.csv',\n",
       " 'Landmark_t_67c3f7ce5eab8804____c1_1____3.csv',\n",
       " 'Landmark_t_67c3f7ce5eab8804____c2_1____3.csv',\n",
       " 'CommonName_t_67c3f7ce5eab8804____c3_0____3.csv',\n",
       " 'Landmark_t_67c3f7ce5eab8804____c4_0____3.csv',\n",
       " 'CommonName_t_67c3f7ce5eab8804____c5_0____3.csv',\n",
       " 'CommonName_t_67c3f7ce5eab8804____c6_0____3.csv',\n",
       " 'Landmark_t_67c3f7ce5eab8804____c6_0____3.csv',\n",
       " 'Landmark_t_67c3f7ce5eab8804____c7_0____3.csv',\n",
       " 'CommonName_t_67c3f7ce5eab8804____c8_0____3.csv',\n",
       " 'Landmark_t_67c3f7ce5eab8804____c8_0____3.csv',\n",
       " 'CommonName_t_67c3f7ce5eab8804____c9_0____3.csv',\n",
       " 'Landmark_t_67c3f7ce5eab8804____c9_0____3.csv',\n",
       " 'Landmark_t_67c3f7ce5eab8804____c9_1____3.csv',\n",
       " 'Department-D�partement_t_c701386b7c10b107____c10_1____2.csv',\n",
       " 'Department-D�partement_t_c701386b7c10b107____c11_0____2.csv',\n",
       " 'Department-D�partement_t_c701386b7c10b107____c11_1____2.csv',\n",
       " 'Department-D�partement_t_c701386b7c10b107____c12_1____2.csv',\n",
       " 'Department-D�partement_t_c701386b7c10b107____c13_0____2.csv',\n",
       " 'Department-D�partement_t_c701386b7c10b107____c14_0____2.csv',\n",
       " 'Department-D�partement_t_c701386b7c10b107____c14_1____2.csv',\n",
       " 'Department-D�partement_t_c701386b7c10b107____c15_1____2.csv',\n",
       " 'Department-D�partement_t_c701386b7c10b107____c16_0____2.csv',\n",
       " 'Department-D�partement_t_c701386b7c10b107____c16_1____2.csv',\n",
       " 'Department-D�partement_t_c701386b7c10b107____c17_0____2.csv',\n",
       " 'Department-D�partement_t_c701386b7c10b107____c18_0____2.csv',\n",
       " 'Department-D�partement_t_c701386b7c10b107____c19_0____2.csv',\n",
       " 'Department-D�partement_t_c701386b7c10b107____c19_1____2.csv',\n",
       " 'Department-D�partement_t_c701386b7c10b107____c20_0____2.csv',\n",
       " 'Department-D�partement_t_c701386b7c10b107____c20_1____2.csv',\n",
       " 'Department-D�partement_t_c701386b7c10b107____c21_0____2.csv',\n",
       " 'Department-D�partement_t_c701386b7c10b107____c21_1____2.csv',\n",
       " 'Department-D�partement_t_c701386b7c10b107____c22_0____2.csv',\n",
       " 'Department-D�partement_t_c701386b7c10b107____c22_1____2.csv',\n",
       " 'Department-D�partement_t_c701386b7c10b107____c23_0____2.csv',\n",
       " 'Department-D�partement_t_c701386b7c10b107____c23_1____2.csv',\n",
       " 'Department-D�partement_t_c701386b7c10b107____c24_0____2.csv',\n",
       " 'Department-D�partement_t_c701386b7c10b107____c8_0____2.csv']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "utils.graph_helpers.get_attribute_of_instance(graph, 'Music Faculty')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run Sherlock on TUS-I Benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.008095440988495952 0.76 0.016020236087689717\n",
      "100 0.005352146940761465 0.88 0.010639584088985613\n",
      "500 0.006248047485160888 0.8 0.012399256044637324\n"
     ]
    }
   ],
   "source": [
    "base_dir=\"output/TUS_injected_homographs/\"\n",
    "cardinality_list, precision_list, recall_list, f1_score_list = [], [], [], []\n",
    "\n",
    "for dir in sorted(os.listdir(base_dir)):\n",
    "    gt_homographs = set()\n",
    "    for i in range(1,51):\n",
    "        gt_homographs.add(\"InjectedHomograph\"+str(i))\n",
    "\n",
    "    cardinality=int(dir.split('_')[1])\n",
    "    if os.path.isfile(base_dir+dir+'/predicted_homographs.pickle'): \n",
    "        predicted_homographs = pd.read_pickle(base_dir+dir+'/predicted_homographs.pickle')\n",
    "\n",
    "        precision = len(gt_homographs & predicted_homographs) / len(predicted_homographs)\n",
    "        recall = len(gt_homographs & predicted_homographs) / len(gt_homographs)\n",
    "        f1_score = (2* precision * recall) / (precision + recall)\n",
    "\n",
    "        print(cardinality, precision, recall, f1_score)\n",
    "\n",
    "        cardinality_list.append(cardinality)\n",
    "        precision_list.append(precision)\n",
    "        recall_list.append(recall)\n",
    "        f1_score_list.append(f1_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Cardinality:\", cardinality_list)\n",
    "print(\"Precision:\", precision_list)\n",
    "print(\"Recall:\", recall_list)\n",
    "print(\"F1-Score:\", f1_score_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.006248047485160888\n",
      "Recall: 0.8\n",
      "F1-Score: 0.012399256044637324\n"
     ]
    }
   ],
   "source": [
    "# gt_homographs = set()\n",
    "# for i in range(1,51):\n",
    "#     gt_homographs.add(\"InjectedHomograph\"+str(i))\n",
    "\n",
    "\n",
    "# precision = len(gt_homographs & predicted_homographs) / len(predicted_homographs)\n",
    "# recall = len(gt_homographs & predicted_homographs) / len(gt_homographs)\n",
    "# f1_score = (2* precision * recall) / (precision + recall)\n",
    "\n",
    "# print(\"Precision:\", precision)\n",
    "# print(\"Recall:\", recall)\n",
    "# print(\"F1-Score:\", f1_score)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 ('env': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.14"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c59dafccc18aa2e3343891f5475ed875e52be3f057901d8e46ce68e9467ac8fd"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
