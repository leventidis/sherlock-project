{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pickle5 as pickle\n",
    "import json\n",
    "import utils\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pyarrow as pa\n",
    "import networkx as nx\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "from sherlock import helpers\n",
    "from sherlock.deploy.model import SherlockModel\n",
    "from sherlock.functional import extract_features_to_csv\n",
    "from sherlock.features.paragraph_vectors import initialise_pretrained_model, initialise_nltk\n",
    "from sherlock.features.preprocessing import (\n",
    "    extract_features,\n",
    "    convert_string_lists_to_lists,\n",
    "    prepare_feature_extraction,\n",
    "    load_parquet_values,\n",
    ")\n",
    "from sherlock.features.word_embeddings import initialise_word_embeddings\n",
    "\n",
    "%load_ext autoreload"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialize Sherlock Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing feature extraction by downloading 4 files:\n",
      "        \n",
      " ../sherlock/features/glove.6B.50d.txt, \n",
      " ../sherlock/features/par_vec_trained_400.pkl.docvecs.vectors_docs.npy,\n",
      "        \n",
      " ../sherlock/features/par_vec_trained_400.pkl.trainables.syn1neg.npy, and \n",
      " ../sherlock/features/par_vec_trained_400.pkl.wv.vectors.npy.\n",
      "        \n",
      "All files for extracting word and paragraph embeddings are present.\n",
      "Initialising word embeddings\n",
      "Initialise Word Embeddings process took 0:00:02.651290 seconds.\n",
      "Initialise Doc2Vec Model, 400 dim, process took 0:00:01.878260 seconds. (filename = ../sherlock/features/par_vec_trained_400.pkl)\n",
      "Initialised NLTK, process took 0:00:00.000464 seconds.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/aristotle/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/aristotle/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "prepare_feature_extraction()\n",
    "initialise_word_embeddings()\n",
    "initialise_pretrained_model(400)\n",
    "initialise_nltk()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the pre-trained sherlock model with its default parameters \n",
    "model = SherlockModel();\n",
    "model.initialize_model_from_json(with_weights=True, model_id=\"sherlock\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use Sherlock to predict the semantic types for each cell node"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_column_node_to_semantic_type_dict(data_dir):\n",
    "    '''\n",
    "    Given a directory `data_dir` containing a list of csv (i.e., tables) files use Sherlock\n",
    "    to predict the semantic type for each column node (i.e, for each column in each specified table)\n",
    "    '''\n",
    "    # Dictionary mapping each column node to the predicted label using Sherlock\n",
    "    column_node_to_semantic_type_dict = {}\n",
    "\n",
    "    for filename in tqdm(os.listdir(data_dir)):\n",
    "        df = pd.read_csv(data_dir+filename, keep_default_na=False, dtype=str)\n",
    "        column_names=df.columns\n",
    "        \n",
    "        # Convert dataframe into a pandas series to be used as input for Sherlock\n",
    "        rows = [df[column_name].tolist() for column_name in column_names]\n",
    "        data_series = pd.Series(rows, name='values')\n",
    "        \n",
    "        # Extract features for the current table and save them at temporary.csv\n",
    "        a = extract_features(\"../temporary.csv\", data_series)\n",
    "        feature_vectors = pd.read_csv(\"../temporary.csv\", dtype=np.float32)\n",
    "        \n",
    "        # Use Sherlock to predict the semantic types for each column\n",
    "        predicted_labels = model.predict(feature_vectors, \"sherlock\")\n",
    "        for column_name, label in zip(column_names, predicted_labels):\n",
    "            column_node_to_semantic_type_dict[column_name+'_'+filename] = label\n",
    "\n",
    "        print(column_names, predicted_labels)\n",
    "\n",
    "    return column_node_to_semantic_type_dict\n",
    "\n",
    "def get_cell_node_to_semantic_type(graph, column_node_to_semantic_type_dict):\n",
    "    '''\n",
    "    Return a dictionary mapping each cell value in a graph to the list of its semantic types found using\n",
    "    Sherlock.\n",
    "    '''\n",
    "    # Extract all cell nodes from the graph\n",
    "    cell_nodes = {n for n, d in graph.nodes(data=True) if d['type']=='cell'}\n",
    "    cell_node_to_semantic_type_dict = {}\n",
    "\n",
    "    # For each cell node find all the column nodes it is connected to and extract their semantic types\n",
    "    for node in cell_nodes:\n",
    "        semantic_types = set()\n",
    "        column_nodes = utils.graph_helpers.get_attribute_of_instance(graph, instance_node=node)\n",
    "        for column_node in column_nodes:\n",
    "            semantic_types.add(column_node_to_semantic_type_dict[column_node])\n",
    "        \n",
    "        cell_node_to_semantic_type_dict[node]=semantic_types\n",
    " \n",
    "    return cell_node_to_semantic_type_dict\n",
    "\n",
    "def get_predicted_homographs(cell_node_to_semantic_type_dict):\n",
    "    '''\n",
    "    Returns a set of the cell node that were predicted to be homographs \n",
    "    (i.e., they were assigned to more than 1 semantic type)\n",
    "    '''\n",
    "    predicted_homographs = set()\n",
    "    for cell_node in cell_node_to_semantic_type_dict:\n",
    "        if len(cell_node_to_semantic_type_dict[cell_node]) > 1:\n",
    "            predicted_homographs.add(cell_node)\n",
    "    return predicted_homographs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Sherlock on the Synthetic Benchmark (SB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/13 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exporting 1588 column features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting Features: 100%|██████████| 3/3 [00:00<00:00,  7.61it/s]\n",
      "  8%|▊         | 1/13 [00:00<00:07,  1.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['animal_name', 'animal_scientific_name', 'country'], dtype='object') ['species' 'species' 'country']\n",
      "Exporting 1588 column features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting Features: 100%|██████████| 2/2 [00:00<00:00,  7.55it/s]\n",
      "Extracting Features: 100%|██████████| 2/2 [00:00<00:00, 46.80it/s]\n",
      " 23%|██▎       | 3/13 [00:01<00:04,  2.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['animal_name', 'animal_scientific_name'], dtype='object') ['species' 'species']\n",
      "Exporting 1588 column features\n",
      "Index(['state', 'state_abbrev'], dtype='object') ['state' 'state']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exporting 1588 column features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting Features: 100%|██████████| 3/3 [00:00<00:00,  8.19it/s]\n",
      " 31%|███       | 4/13 [00:01<00:03,  2.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['company_name', 'full_name', 'country'], dtype='object') ['company' 'name' 'country']\n",
      "Exporting 1588 column features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting Features: 100%|██████████| 3/3 [00:00<00:00,  7.20it/s]\n",
      " 38%|███▊      | 5/13 [00:02<00:03,  2.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['plant_name', 'plant_scientific_name', 'plant_family'], dtype='object') ['name' 'name' 'family']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting Features: 100%|██████████| 2/2 [00:00<00:00,  5.63it/s]\n",
      " 46%|████▌     | 6/13 [00:02<00:03,  2.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exporting 1588 column features\n",
      "Index(['product_grocery', 'country'], dtype='object') ['description' 'country']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exporting 1588 column features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting Features: 100%|██████████| 4/4 [00:00<00:00,  9.93it/s]\n",
      " 54%|█████▍    | 7/13 [00:02<00:02,  2.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['car_make', 'car_model', 'car_model_year', 'country_code'], dtype='object') ['brand' 'name' 'year' 'country']\n",
      "Exporting 1588 column features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting Features: 100%|██████████| 5/5 [00:00<00:00, 15.32it/s]\n",
      " 62%|██████▏   | 8/13 [00:03<00:02,  2.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['first_name', 'last_name', 'ssn', 'gender', 'country'], dtype='object') ['name' 'name' 'address' 'sex' 'country']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting Features: 100%|██████████| 3/3 [00:00<00:00,  8.85it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exporting 1588 column features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 69%|██████▉   | 9/13 [00:03<00:01,  2.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['movie_title', 'movie_genre', 'country_code'], dtype='object') ['name' 'genre' 'country']\n",
      "Exporting 1588 column features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting Features: 100%|██████████| 4/4 [00:00<00:00, 13.51it/s]\n",
      "Extracting Features: 100%|██████████| 2/2 [00:00<00:00, 11.87it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['full_name', 'credit_card_type', 'credit_card', 'email_address'], dtype='object') ['name' 'class' 'address' 'address']\n",
      "Exporting 1588 column features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 85%|████████▍ | 11/13 [00:04<00:00,  2.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['city', 'country'], dtype='object') ['city' 'country']\n",
      "Exporting 1588 column features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting Features: 100%|██████████| 4/4 [00:00<00:00,  7.67it/s]\n",
      "Extracting Features: 100%|██████████| 2/2 [00:00<00:00, 14.99it/s]\n",
      "100%|██████████| 13/13 [00:05<00:00,  2.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['plant_name', 'plant_scientific_name', 'plant_family', 'country'], dtype='object') ['name' 'name' 'family' 'country']\n",
      "Exporting 1588 column features\n",
      "Index(['country', 'country_code'], dtype='object') ['country' 'country']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "data_dir=\"data/synthetic_benchmark/\"\n",
    "graph = pickle.load(open('graph_representations/synthetic_benchmark/bipartite.graph', 'rb'))\n",
    "\n",
    "# Groundtruth Homographs\n",
    "gt = pd.read_pickle('ground_truth/synthetic_example_groundtruth_dict.pickle')\n",
    "gt_homographs = set([val for val in gt if gt[val]=='homograph'])\n",
    "\n",
    "# Find the semantic types for each column\n",
    "column_node_to_semantic_type_dict = get_column_node_to_semantic_type_dict(data_dir)\n",
    "\n",
    "# Find the semantic types for each cell node\n",
    "cell_node_to_semantic_type_dict = get_cell_node_to_semantic_type(graph, column_node_to_semantic_type_dict)\n",
    "predicted_homographs = get_predicted_homographs(cell_node_to_semantic_type_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 1.0\n",
      "Recall: 0.6909090909090909\n",
      "F1-Score: 0.8172043010752689\n"
     ]
    }
   ],
   "source": [
    "precision = len(gt_homographs & predicted_homographs) / len(predicted_homographs)\n",
    "recall = len(gt_homographs & predicted_homographs) / len(gt_homographs)\n",
    "f1_score = (2* precision * recall) / (precision + recall)\n",
    "\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Recall:\", recall)\n",
    "print(\"F1-Score:\", f1_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GT homographs not predicted by Sherlock: {'Costanza', 'Crossfire', 'Christophe', 'Nadine', 'Leandra', 'Jimmy', 'Berkeley', 'Duff', 'Vinson', 'Reid', 'Smitty', 'Heather', 'Garvey', 'Conroy', 'Else', 'Charity', 'Elan'}\n"
     ]
    }
   ],
   "source": [
    "print(\"GT homographs not predicted by Sherlock:\", gt_homographs - predicted_homographs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Sherlock on TUS Benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir=\"data/TUS/csvfiles/\"\n",
    "graph = pickle.load(open('graph_representations/TUS/bipartite.graph', 'rb'))\n",
    "\n",
    "# Groundtruth Homographs\n",
    "with open(\"ground_truth/groundtruth_TUS_short_format.pickle\", \"rb\") as fh:\n",
    "    gt = pickle.load(fh)\n",
    "gt_homographs = set([val for val in gt if gt[val]=='homograph'])\n",
    "\n",
    "# Find the semantic types for each column\n",
    "column_node_to_semantic_type_dict = get_column_node_to_semantic_type_dict(data_dir)\n",
    "\n",
    "# Find the semantic types for each cell node\n",
    "cell_node_to_semantic_type_dict = get_cell_node_to_semantic_type(graph, column_node_to_semantic_type_dict)\n",
    "predicted_homographs = get_predicted_homographs(cell_node_to_semantic_type_dict)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 ('env': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.14"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c59dafccc18aa2e3343891f5475ed875e52be3f057901d8e46ce68e9467ac8fd"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
